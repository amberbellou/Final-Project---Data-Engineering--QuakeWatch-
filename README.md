# ğŸŒ QuakeWatch â€” Real-time Earthquake Data Pipeline & Dashboard

QuakeWatch is a data engineering project that ingests, processes, and visualizes earthquake data in near real time.  
It fetches seismic event data from the **USGS Earthquake API**, processes and validates it, stores it in a database, exposes it via a **FastAPI service**, and provides an interactive **Streamlit dashboard** for exploration.

---

## ğŸ“– Project Overview

QuakeWatch is a real-time earthquake monitoring system that collects, processes, and visualizes global seismic activity.  
The goal is to provide a clean, structured dataset and a live dashboard for researchers, analysts, and the public to track earthquakes as they happen.

The pipeline runs automatically every **6 hours** to keep the data fresh.  
This project was designed to demonstrate a full ETL lifecycle, automation with cron, and modern visualization techniques.

---

## ğŸ¯ Purpose and Use Cases
QuakeWatch is useful for:
- ğŸ§‘â€ğŸ”¬ Researchers studying seismic trends over time.  
- ğŸ›ï¸ Governments & disaster-response teams monitoring earthquake activity.  
- ğŸ“ Educators and students learning about seismology and data pipelines.  
- ğŸŒ Anyone curious about real-time global earthquake activity.  

---

## ğŸ“Œ System Architecture

### 1. ETL Pipeline
- **Extract**: Pulls earthquake event data from the USGS feed.  
- **Transform**: Cleans and validates the data (e.g., country parsing, Pandera schema checks).  
- **Load**: Stores processed events into a local **SQLite database** (PostgreSQL ready).  

### 2. API (FastAPI)
- REST endpoints for:
  - Recent earthquake events  
  - Statistics by country  
  - Service health checks  
- Auto-documented with Swagger UI at **`/docs`**.  

### 3. Dashboard (Streamlit)
- Interactive web app for real-time insights.  
- Includes:
  - ğŸŒ Map of events  
  - ğŸ“Š Magnitude timeline  
  - ğŸ“‹ Recent events table  
  - ğŸ† Top countries by quake count  
  - Filters for magnitude thresholds and row limits.  

---

## ğŸ›  Tech Stack

| Component       | Technology                                                         |
| --------------- | ------------------------------------------------------------------ |
| Data Source     | [USGS Earthquake API](https://earthquake.usgs.gov/fdsnws/event/1/) |
| Database        | SQLite (local) / PostgreSQL (optional)                             |
| API             | FastAPI                                                            |
| Dashboard       | Streamlit                                                          |
| Orchestration   | Cron (local) / Prefect (optional)                                  |
| Language        | Python 3.11                                                        |
| ORM             | SQLAlchemy                                                         |
| Data Processing | Pandas + Pandera                                                   |

---

## ğŸ“‚ Project Structure

Final-Project---Data-Engineering--QuakeWatch-/

â”œâ”€â”€ app/ # API & Dashboard code

â”‚ â”œâ”€â”€ api.py # FastAPI service

â”‚ â”œâ”€â”€ dashboard.py # Streamlit dashboard

â”‚ â”œâ”€â”€ db.py # Database connection

â”‚ â””â”€â”€ models.py # ORM models

â”‚

â”œâ”€â”€ etl/ # ETL pipeline code

â”‚ â”œâ”€â”€ extract.py # Fetch data

â”‚ â”œâ”€â”€ transform.py # Clean & validate

â”‚ â”œâ”€â”€ load.py # Load into DB

â”‚ â””â”€â”€ flow.py # Orchestration

â”‚

â”œâ”€â”€ templates/ # HTML templates

â”‚ â””â”€â”€ events.html

â”‚

â”œâ”€â”€ tests/ # Unit tests

â”‚ â”œâ”€â”€ test_api.py

â”‚ â””â”€â”€ test_transform.py

â”‚

â”œâ”€â”€ requirements.txt # Python dependencies

â”œâ”€â”€ docker-compose.yml # Optional multi-service setup

â”œâ”€â”€ Makefile # Automation commands

â”œâ”€â”€ local.db # Local SQLite DB

â”œâ”€â”€ etl_cron.log # Logs from cron runs

â””â”€â”€ README.md # Documentation



---
## ğŸ“¸ Screenshots

### API Swagger UI
![Swagger UI](https://github.com/user-attachments/assets/ee5f71e5-c78d-4d51-95a9-3486f1ba38d0)

### API Health Check
![Health Check](https://github.com/user-attachments/assets/5cd99f0c-d48a-4821-ad54-0a706c6aa363)

### JSON Events Endpoint
![Events JSON](https://github.com/user-attachments/assets/937d4601-32ea-4fe0-8769-efca7817af55)

### Stats by Country Endpoint
![Stats by Country](https://github.com/user-attachments/assets/193a7068-ffc5-4082-88ec-d6de095ecf22)

### Dashboard â€“ Top Countries by Quake Count
![Dashboard by Country](https://github.com/user-attachments/assets/1d7a759b-3974-46f2-a732-4177036abb84)

### Dashboard â€“ Recent Trends
![Recent Trends](https://github.com/user-attachments/assets/a969d52e-0cf2-4e91-893e-b82236c418f2)

### Dashboard â€“ World Map
![Global Map](https://github.com/user-attachments/assets/66908318-8de1-431b-b511-ec1235d3dc2d)

---

## ğŸš€ How to Run Locally

```bash
# 1ï¸âƒ£ Clone the repository
git clone https://github.com/<yourusername>/quakewatch.git
cd quakewatch

# 2ï¸âƒ£ Create and activate a virtual environment
python3 -m venv .venv
source .venv/bin/activate

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Run ETL manually
DATABASE_URL=sqlite:///./local.db python -m etl.flow

# 5ï¸âƒ£ Start API (port 8001)
uvicorn app.api:app --reload --port 8001

# 6ï¸âƒ£ Start Dashboard (port 8505)
streamlit run app/dashboard.py --server.port 8505

# 7ï¸âƒ£ Verify cron job
crontab -l

ğŸ“Š Example API Calls

# Health check
curl http://localhost:8001/health

# Recent events
curl -s "http://localhost:8001/events.json?min_mag=4&limit=3" | jq .

# Stats by country
curl -s "http://localhost:8001/stats/by-country?min_mag=5" | jq .

#

âœ… Features

Automated ETL pipeline (runs every 6 hours).
Database-backed API for structured earthquake data.
Interactive Streamlit dashboard with maps, charts, and filters.
Cron-based scheduling with logs.
Modular, extensible design for future scaling.

ğŸ“… Future Improvements

Historical trend analysis.
Advanced filtering (date ranges, depth).
Real-time alerts for major events.
Deployment to cloud (AWS/GCP/Render).

ğŸ‘©â€ğŸ’» Author
Developed by Amber Bellou
This project was built as my Data Engineering final project, combining ETL, APIs, and dashboards into a single, automated pipeline.
